{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**learning problem** = ![](other/learning-problem.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$\\lambda$** =parameter, determines the trade-off between increasing the margin size and ensuring that the  ${\\vec {x}}_{i}$ lie on the correct side of the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**soft-margin** = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$\\zeta _{i}$** ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hinge loss function** ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**overfitting**="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**kernel function** : A kernel function K is a nonnegative function\n",
    "K : <\n",
    "n × <n × <+ → <+\n",
    "where the first argument is a n-dimensional input, the second argument is typically\n",
    "called the center and the third argument is called width or bandwidth. Once a\n",
    "distance function between the input and the center\n",
    "d : <\n",
    "n × <n → <+ (C.0.1)\n",
    "is defined, the kernel function can be expressed as a function\n",
    "K : <\n",
    "+ × <+ → <+ (C.0.2)\n",
    "of the distance d and the bandwidth parameter. The maximum value of a kernel\n",
    "function is located at zero distance and the function decays smoothly as the distance\n",
    "increases\n",
    "\n",
    "Here is some examples of Kernel function: Inverse distance, Corrected inverse distance, Gaussian kernel, Exponential kernel, \n",
    "\n",
    "\n",
    "etc\n",
    "\n",
    "kernel method\n",
    "In machine learning, kernel methods are a class of algorithms for pattern analysis, whose best known member is the support vector\n",
    "machine (SVM). The general task of pattern analysis is to find and study general types of relations (for example clusters, rankings,\n",
    "principal components, correlations, classifications) in datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**early-stopping**: Early stopping is a technique for avoiding overfitting when training a machine learning model with iterative method. We set the early stopping in such a way that when the performance has stopped improving on the held-out validation set, the model training stops.\n",
    "For example, in XGBoost, as you train more and more trees, you will overfit your training dataset. Early stopping enables you to specify a validation dataset and the number of iterations after which the algorithm should stop if the score on your validation dataset didn’t increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hyper parmeter** : A hyperparameter is a parameter whose value is set before training a machine learning or deep learning model. Different models require different hyperparameters and some require none. Hyperparameters should not be confused with the parameters of the model because the parameters are estimated or learned from the data.\n",
    "Some keys points about the hyperparameters are:\n",
    "\n",
    "They are often used in processes to help estimate model parameters.\n",
    "They are often manually set.\n",
    "They are often tuned to tweak a model’s performance\n",
    "Number of trees in a Random Forest, eta in XGBoost, and k in k-nearest neighbours are some examples of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hyperplane**: It is a subspace with one fewer dimensions than its surrounding area. If a space is 3-dimensional then its hyperplane is just a normal 2D plane. In 5 dimensional space, it’s a 4D plane, so on and so forth.\n",
    "Most of the time it’s basically a normal plane, but in some special cases, like in Support Vector Machines, where classifications are performed with an n-dimensional hyperplane, the n can be quite large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**overfitting**:\tA model is said to overfit when it performs well on the train dataset but fails on the test set. This happens when the model is too sensitive and captures random patterns which are present only in the training dataset. There are two methods to overcome overfitting:\n",
    "Reduce the model complexity\n",
    "Regularization\n",
    "\n",
    "On the opposite, Underfitting occurs when a statistical model or machine learning algorithm cannot capture the underlying trend of the data. It refers to a model that can neither model on the training data nor generalize to new data. An underfit model is not a suitable model as it will have poor performance on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN**: K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases by a majority vote of its k neighbors. The case being assigned to the class is most common amongst its K nearest neighbors measured by a distance function.\n",
    "\n",
    "These distance functions can be Euclidean, Manhattan, Minkowski and Hamming distance. First three functions are used for continuous function and fourth one (Hamming) for categorical variables. If K = 1, then the case is simply assigned to the class of its nearest neighbor. At times, choosing the value for K can be a challenge while performing KNN modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM**: \t\n",
    "It is a classification method. In this algorithm, we plot each data item as a point in n-dimensional space (where n is the number of features you have) with the value of each feature being the value of a particular coordinate.\n",
    "\n",
    "For example, if we only have two features like Height and Hair length of an individual, we’d first plot these two variables in two-dimensional space where each point has two coordinates (these coordinates are known as Support Vectors) Now, we will find some line that splits the data between the two differently classified groups of data. This will be the line such that the distances from the closest point in each of the two groups will be farthest away.\n",
    "\n",
    "more on that: svm.pdf in ref/fragment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**machine learning procedure**:  \n",
    "\n",
    "6.1 Introduction . . . . . . . . . .   . . . . \n",
    "\n",
    "6.2 Problem formulation . . . . . . . . .  . . \n",
    "\n",
    "6.3 Experimental design . . . . . . .  . . . . \n",
    "\n",
    "6.4 Data pre-processing . . . . . . . . . . . .\n",
    "\n",
    "6.5 The dataset . . . . . . . . . . . . . . . .\n",
    "\n",
    "6.6 Parametric identification . . . . . . . . .\n",
    "\n",
    "6.6.1 Error functions . . . . . . . . . . . . .\n",
    "\n",
    "6.6.2 Parameter estimation . . . . . . . . . . \n",
    "\n",
    "6.7 Structural identification . . . . . . . . .\n",
    "\n",
    "6.7.1 Model generation . . . . . . . . . . . . \n",
    "\n",
    "6.7.2 Validation . . . . . . . . . . . . . . . \n",
    "\n",
    "6.7.3 Model selection criteria . . . . . . . . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
