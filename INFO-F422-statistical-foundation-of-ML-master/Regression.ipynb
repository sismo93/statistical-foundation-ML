{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fundamentally, we have study two predictive model the first one is about classification, and we have seen that it is about predicting a label, and we will now investigate how we will solve another kind of problem, regression and this time it will be about predicting a quantity.\n",
    "\n",
    "Predictive modeling is the problem of developing a model using data-set to make a prediction on new data where we do not have the answer.\n",
    "\n",
    "Predictive modeling can be described as the mathematical problem of approximating a mapping function (f) from input variables (X) to output variables (y).\n",
    "\n",
    "Regression predictive modeling is the task of approximating a mapping function (f) from input variables (X) to a continuous output variable (y).\n",
    "\n",
    "A continuous output variable is a real-value, such as an integer or floating-point value. These are often quantities, such as amounts and sizes.\n",
    "\n",
    "We have chosen to do house price prediction for different reason but the main one is that they are some dataset available.\n",
    "\n",
    "We will use modeling algorithms that will find the best possible mapping function given the time and resources available. In this notebook  the regression will be solved using linear regression and a neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of simple regression is to explain a variable Y using a variable X.\n",
    "\n",
    "-the variable Y is called dependent variable in our case it will be the price of the house \n",
    "\n",
    "-the variables Xj are called independent variable in our case it could be the neighborhood, the built year, if it has a pool or not, etc.\n",
    "\n",
    "We rarely estimate a home using only one parameter. We define $x_j$ from the set of parameter $X\\hspace{3mm} \\forall(j = 1, ..., q)$ that describe the house. \n",
    "\n",
    "As said in the introduction, the regression would be solved using a linear regression algorithm and a neural network, although there are many algorithms other than this two for this particular task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\wassi\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b9f74a135563>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wassi\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     raise ImportError(\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[1;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n",
      "\u001b[1;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "#import the librairie\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import binned_statistic\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is to load the data then we do some analysis , the first thing we notice there are some  holes in same cells\n",
    "\n",
    "Missing data will leads to a depreciation of our predictive models and will bring some lacks of accuracy because its will give some wrong information to our algorithms and bring some noise in output graphs. \n",
    "\n",
    "The solution is to do some cleaning, we have to eliminate every missing cells, that will reduce our data set but brings him more pertinence \n",
    "\n",
    "Before cleaning the data we will do some analysis to see the correlations between variable and  there inside, that will increase the quality of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/regression/train.csv') \n",
    "train.head(15)\n",
    "train['SalePrice'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('dataset/regression/test.csv')\n",
    "test.head(15)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check if the data set has any missing values. \n",
    "train.columns[train.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing value counts in each of these columns\n",
    "missing_value = train.isnull().sum()\n",
    "\n",
    "missing_value.sort_values(inplace=True, ascending=False)\n",
    "missing_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert into dataframe\n",
    "missing_value_frame = missing_value.to_frame()\n",
    "\n",
    "missing_value_frame.columns = ['count']\n",
    "\n",
    "missing_value_frame.index.names = ['Parametre']\n",
    "\n",
    "missing_value_frame['Parametre'] = missing_value_frame.index\n",
    "\n",
    "#plot Missing values\n",
    "plt.figure(figsize=(13, 5))\n",
    "sns.set(style='whitegrid')\n",
    "sns.barplot(x='Parametre', y='count', data=missing_value_frame)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are some parameters with a big amount of null cells we will represent that in percentage by parameter to visualize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_missing_value = train.isnull().sum()/len(train)*100\n",
    "percentage_missing_value= percentage_missing_value[percentage_missing_value>0]\n",
    "percentage_missing_value.sort_values(inplace=True, ascending=False)\n",
    "percentage_missing_value = percentage_missing_value.to_frame()\n",
    "\n",
    "percentage_missing_value.columns = ['Percentage']\n",
    "percentage_missing_value.index.names = ['Parametere']\n",
    "\n",
    "percentage_missing_value['Parametre'] = percentage_missing_value.index\n",
    "\n",
    "#plot Missing values\n",
    "plt.figure(figsize=(13, 5))\n",
    "sns.set(style='whitegrid')\n",
    "sns.barplot(x='Parametre', y='Percentage', data=percentage_missing_value)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here, there 7 parameters with more than 50 % and 16 parameters with missing value. We have to do some choice and eliminate some columns of our data set. To help us   decide we will analyze the correlation between our dependent variable the sale price and our independent variable the other columns by outputting a correlation matrix. This is a statistical tool that will give use a good view on which parameter to keep we keep only the parameter with a  correlation parameter >0.7. We should also avoid every column where the type is not a number because it will be unusable for our algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = train.corr()\n",
    "correlation.sort_values(['SalePrice'], ascending=False, inplace=True)\n",
    "correlation.SalePrice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_number_only = train.select_dtypes(include=[np.number])\n",
    "del data_number_only['Id']\n",
    "correlation= data_number_only.corr()\n",
    "\n",
    "top_core_parametre = correlation.index[abs(correlation['SalePrice']>0.3)]\n",
    "plt.subplots(figsize=(12, 8))\n",
    "top_core_matrix = train[top_core_parametre].corr()\n",
    "sns.heatmap(top_core_matrix, annot=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will keep the variable that are the most correlated with our dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# PoolQC has missing value ratio is 99%+. So, there is fill by None\n",
    "train['PoolQC'] = train['PoolQC'].fillna('None')\n",
    "\n",
    "#Arround 50% missing values attributes have been fill by None\n",
    "train['MiscFeature'] = train['MiscFeature'].fillna('None')\n",
    "train['Alley'] = train['Alley'].fillna('None')\n",
    "train['Fence'] = train['Fence'].fillna('None')\n",
    "train['FireplaceQu'] = train['FireplaceQu'].fillna('None')\n",
    "\n",
    "#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\n",
    "train['LotFrontage'] = train.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n",
    "    lambda x: x.fillna(x.median()))\n",
    "\n",
    "#GarageType, GarageFinish, GarageQual and GarageCond these are replacing with None\n",
    "for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n",
    "    train[col] = train[col].fillna('None')\n",
    "\n",
    "#GarageYrBlt, GarageArea and GarageCars these are replacing with zero\n",
    "for col in ['GarageYrBlt', 'GarageArea', 'GarageCars']:\n",
    "    train[col] = train[col].fillna(int(0))\n",
    "\n",
    "#BsmtFinType2, BsmtExposure, BsmtFinType1, BsmtCond, BsmtQual these are replacing with None\n",
    "for col in ('BsmtFinType2', 'BsmtExposure', 'BsmtFinType1', 'BsmtCond', 'BsmtQual'):\n",
    "    train[col] = train[col].fillna('None')\n",
    "\n",
    "#MasVnrArea : replace with zero\n",
    "train['MasVnrArea'] = train['MasVnrArea'].fillna(int(0))\n",
    "\n",
    "#MasVnrType : replace with None\n",
    "train['MasVnrType'] = train['MasVnrType'].fillna('None')\n",
    "\n",
    "#There is put mode value \n",
    "train['Electrical'] = train['Electrical'].fillna(train['Electrical']).mode()[0]\n",
    "\n",
    "#There is no need of Utilities\n",
    "train = train.drop(['Utilities'], axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the string into numbers the models is mathematical, it cannot evaluate a string this is why this step is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_meta = ['SalePrice','GrLivArea', 'MasVnrArea', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'OverallQual', 'Fireplaces', 'GarageCars']\n",
    "train_meta = train[idx_meta].copy()\n",
    "train_meta.head(n=5)\n",
    "idx_meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
    "        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
    "        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n",
    "        'YrSold', 'MoSold', 'MSZoning', 'LandContour', 'LotConfig', 'Neighborhood',\n",
    "        'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n",
    "        'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'Foundation', 'GarageType', 'MiscFeature', \n",
    "        'SaleType', 'SaleCondition', 'Electrical', 'Heating')\n",
    "\n",
    "\n",
    "for c in cols:\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(train[c].values)) \n",
    "    train[c] = lbl.transform(list(train[c].values))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is prepared we will do some analysis on the target function  and check if the distribution of our data fits a normal distribution \n",
    "\n",
    "In statistic normal distribution is in probability theory, a normal (or Gaussian or Gauss or Laplace–Gauss) distribution is a type of continuous probability distribution for a real-valued random variable.  \n",
    "\n",
    "The Normal Distribution or often called the Gaussian distribution  is one of the most important and commonly-encountered probability distribution.\n",
    "Any time you add together a large amount of random variables, even if those variables are from different distributions, if you get enough samples you'll find that the sum of the variables tends to be normally distributed. In our case the variable are the parameter that will influence the price of the house. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,10))\n",
    "sns.distplot(train['SalePrice'], fit=stats.norm)\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "\n",
    "(mu, sigma) = stats.norm.fit(train['SalePrice'])\n",
    "\n",
    "# plot with the distribution vs normal distribution \n",
    "\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "fig = plt.figure()\n",
    "stats.probplot(train['SalePrice'], plot=plt)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, it’s normal to have different errors from various sources, from data corruption to classification errors. While it’s important to always check to ensure your assumption is correct, it’s not unreasonable to think that the combined effect of these errors is approximately normal. \n",
    "\n",
    "Accepting the normal distribution also makes the math easier and faster to do, which is an important consideration when teaching AI. Due to the simplicity of normal distribution. Here we can see that our distribution don't fit the normal one to solve this we will use a log scale to represent the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "train['SalePrice'] = np.log1p(train['SalePrice'])\n",
    "\n",
    "\n",
    "plt.subplots(figsize=(12,9))\n",
    "sns.distplot(train['SalePrice'], fit=stats.norm)\n",
    "\n",
    "\n",
    "(mu, sigma) = stats.norm.fit(train['SalePrice'])\n",
    "\n",
    "# plot with the distribution\n",
    "\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "#Probablity plot\n",
    "\n",
    "fig = plt.figure()\n",
    "stats.probplot(train['SalePrice'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preditiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lineair regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is perhaps one of the most well known and well-understood algorithms in statistics and machine learning. Linear regression was developed in the field of statistics and is studied as a model for understanding the relationship between input and output numerical variables.\n",
    "\n",
    "In statistics, linear regression is a linear approach to modeling the relationship between a dependent variable and one or more independent variables.\n",
    "The relationships are modeled using linear predictor functions $h(x)$. To perform regression, you must decide the way you are going to represent $h$. As an initial choice, let’s say you decide to approximate $y$ as a linear function of $x$ :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$ h_θ(x) = θ_0 + θ_1x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the $θ_i$’s are the parameters (the weights in our neural network) parameterizing the space of linear functions mapping from $X$ to $Y$. In simple words, these parameters are used for accurately mapping $X$ the independent variable to $Y$, the predicted sale price, we will also introduce the $x_0$ , this is the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](other/h.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the main question is how we will pick or learn the parameters $θ$ we cannot change your input instances as to predict the prices. You have only these $θ$ parameters to modify.\n",
    "\n",
    "One prominent method seems to be to make $h(x)$ close to y, at least for the training examples we have. To understand this more formally, let's try defining a function that determines, for each value of the $θ$’s, how close the $h(x(i))$’s are to the corresponding $y(i)$’s, it is a minimization problem of the objective function.\n",
    "\n",
    "We provide a mathematical formulation of the function with a statistical tool called the mean squared value :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{j} (x{j}-pre{j})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It measures the average of the squares of the errors — that is, the average squared difference between the estimated values and the actual value. MSE is a risk function, corresponding to the expected value of the squared error loss. It is called as cost function in machine learning\n",
    "\n",
    "There are a several way to model the loos function like the Mean Absolute Error, but we choose the most efficient one and the must used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](other/m.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It is important to note that, linear regression can often be divided into two basic forms:\n",
    "\n",
    "- Simple Linear Regression (SLR) which deals with just two variables \n",
    "- Multi-linear Regression (MLR) which deals with more than two variables - this is our case  \n",
    "\n",
    "Now we will focus more about the ways of estimating the parameters. This estimation of parameters is essentially known as the training of linear regression. There are many methods to train a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take targate variable into y\n",
    "y = train['SalePrice']\n",
    "\n",
    "#Delete the saleprice\n",
    "del train['SalePrice']\n",
    "\n",
    "#Take their values in X and y\n",
    "X = train.values\n",
    "y = y.values\n",
    "\n",
    "# Split data into train and test formate\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used Ordinary least squares Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = linear_model.LinearRegression()\n",
    "\n",
    "#Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "yd_lm = model.predict(X_test)\n",
    "\n",
    "print(\"Predict value \" + str(model.predict([X_test[142]])))\n",
    "print(\"Real value \" + str(y_test[142]))\n",
    "\n",
    "print(\"Accuracy --> \", model.score(X_test, y_test)*100)\n",
    "\n",
    "sns.regplot(y_test, yd_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use another model to do house price prediction we will this time use a deep neural network combined with an optimization algorithm to train it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A deep neural network is an artificial neural network with multiple layers between the input and output layers called hidden layers. The DNN finds the correct mathematical manipulation to turn the input into the output, for linear relationship and non-linear relationship.\n",
    "\n",
    "DNNs are typically feed forward networks in which data flows from the input layer to the output layer without looping back. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feed-forward neural network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A feed-forward neural network  It consist of a number of simple processing units often called perceptron, organized in layers.\n",
    "\n",
    "The structure of the connections between layers are very simple each perceptron in a layer is connected with all the units in the previous layer.\n",
    " \n",
    "These connections have different value: they may have a different strength or weight. The weights on these connections encode the knowledge of a network.\n",
    "\n",
    "Data enters at the inputs and passes through the network, layer by layer, until it arrives at the outputs.  There is no feedback between layers only one  way. This is why they are called feed-forward neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](other/c.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial neuron or perceptron simply calculates a “weighted sum” of its input (y), add a bias and then decides whether it activate or not the neuron. Finally, the activation function performs this decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$\\sum (weight*input)+bias $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a neural network, the activation function is responsible for transforming the summed weighted input from the node into the activation of the node or output for that input.here job is to create the final patterns that will to the right model \n",
    "\n",
    "The most common activation function are the sigmoid and hyperbolic tangent, but cannot be used in networks with many layers due to the vanishing gradient problem. The rectified linear activation function overcomes the vanishing gradient problem, allowing models to learn faster and perform better.\n",
    "\n",
    "The rectified linear activation is the default activation when developing multilayer Perceptron\n",
    "\n",
    "It used to determine the output of the neural network if yes or no, the resulting value it’s always between 0 and 1 or -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](other/1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ReLU (Rectified linear unit) is the most used activation function in the world right now. Since, it is used in almost all the deep neural networks of deep learning.  The mathematical formulation for ReLU is  $y = max(0,x)$.\n",
    "\n",
    "It's a piece wise linear function that will output the same input value directly if it is positive, otherwise, it will output zero. It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fulfillment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network model goes through 2 main phases :\n",
    "\n",
    "-The first allows the diffusion of the information from the layer of input parameter to the deeper layers, this is ensured by the feed forward algorithm, it aims to try to arrive at the right target.\n",
    "\n",
    "-The second phase consists of optimization the function using an algorithm and with calculating the cost with a sum-squared method we will talk about this later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent and Adam "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier we talked about supervised learning. In this kind of learning we have a 2 groups of  data-set that has been labeled for training the neural network and the second for the test data-set and will be used to verify how good the trained network predict unseen data.\n",
    "\n",
    "When training our neural network we feed sample by sample from the training data-set through the network and for each of these we inspect the outcome. In particular, we check how much the outcome differs from what we expected (the label). The difference between what we expected and what we got is called the error or loss. The loss tells us how much we differ from reality, This measure can then be used to adjust the network slightly so that it will be less wrong the next time the\n",
    "\n",
    "There are several cost functions that can be used in our project we used the quadratic cost function: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{j} (x{j}-pre{j})^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, the mean squared error measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the actual value.we will. This loss functions evaluate how well our line fits the data for our project the sum of:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$ \\text{sum squared residual} = (\\text{observed digit}  -\\text{predicted digit} )^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted sales price fit a line of equation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{pred} = \\text{intercept} + \\text{slope}*x$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we replace predicted value in the first equation we can calculate two derivative:\n",
    "\n",
    "the derivative of pred with respect to intercept, that will lead us to a number that will define the step size of the intercept after that we can calculate  the new intercept according to the equation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  \\text{stepsizeintercept} = (\\frac{\\partial pred }{\\partial inter })*\\text{LearningRate} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then calculate the value of the new intercept: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\\text{newintercept}=\\text{oldintercept}-\\text{stepsizeintercept}  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of pred with respect to slope, that will lead us to a number that will define the step size of the slope. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{ stepsizeslope}  = (\\frac{\\partial pred}{\\partial slope})*\\text{LearningRate} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then calculate the value of the new slope :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$\\text{newslope}=\\text{oldsolve}-\\text{stepsizeslope} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can play on the learning process to take bigger step or smaller one. We advise to take a big number for the value of the learning rate because we are far away from the minimum then reduce it when getting closer to minimum, that will give us result whit more accuracy. \n",
    "\n",
    "By picking randomly a sample for each step and doing the sum squared each time, it will lead us to modification on our prediction line to fit the data as well as possible.\n",
    "\n",
    "This technique reduces the number of terms computed by 3, compared to the gradient descend it more efficient for big amount of data like the one we have. \n",
    "\n",
    "The calculation becomes faster, but the process of gradient descent becomes fluctuating. The direction of gradient descent calculated from only one data is not globally stable. It can be even opposite to the real gradient descent direction.\n",
    "\n",
    "In our model the Stochastic gradient descent can be implemented by the adam optimize, which is predefined in the keras library. We opted for the adam algorithm instead.\n",
    "\n",
    "Adam algorithm is a variant to classical stochastic gradient descent.\n",
    "\n",
    "Stochastic gradient descent maintains a single learning rate for all weight updates and the learning rate does not change during training.\n",
    "\n",
    "Instead, the adam optimizer computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients.\n",
    "\n",
    "Adam is a replacement optimization algorithm for stochastic gradient descent there exist other optimizer that we could use but Adam combines the best properties the AdaGrad and RMSProp algorithms, the two other concurrent, to provide an optimization algorithm that can handle sparse gradients on noisy problems.\n",
    "\n",
    "Adam is relatively easy to configure where the default configuration parameters do well on most problems.\n",
    "\n",
    "    *alpha:the learning rate or step size.\n",
    "    *beta1: The exponential decay rate for the first moment estimates.\n",
    "    *beta2: The exponential decay rate for the second-moment estimates.    \n",
    "    *epsilon.:Is a very small number to prevent any division by zero in the implementation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_nn_model(dims):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(dims, input_dim=dims,kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def larger_nn_model(dims):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(dims, input_dim=dims,kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(35, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(15, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_keras_nn_model(nn_model, x, y, xx, yy, epoch, ylim):\n",
    "\n",
    "    yy_predict =nn_model.predict(xx)\n",
    " \n",
    "    ax = sns.regplot(yy, yy_predict)\n",
    "\n",
    "    ax.set(ylim=ylim)\n",
    "    plt.show()   \n",
    "use_keras_nn_model(baseline_nn_model(X_train.shape[1]), X_train, y_train, X_test, y_test, 700,  (-500, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_largernn = use_keras_nn_model(larger_nn_model(X_train.shape[1]), X_train, y_train, X_test, y_test, 500, (-30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "261.875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
